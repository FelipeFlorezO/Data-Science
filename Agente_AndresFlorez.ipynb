{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AW6Tl3ZjMH-l"
      },
      "source": [
        "Estudiante: Andrés Felipe Flórez Olivera\n",
        "Introducción a la Inteligencia Artificial - Semana 7 - Actividad 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nvtQ4q3KGLN",
        "outputId": "5942f24d-2b7c-432a-aed4-af672e0e7bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Episode 1 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 1 completado\n",
            "\n",
            "Episode 2 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 2 completado\n",
            "\n",
            "Episode 3 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n",
            "Episode 3 completado\n",
            "\n",
            "Episode 4 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 4 completado\n",
            "\n",
            "Episode 5 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 5 completado\n",
            "\n",
            "Episode 6 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n",
            "Episodio: 7, Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 6 completado\n",
            "\n",
            "Episode 7 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n",
            "Episode 7 completado\n",
            "\n",
            "Episode 8 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 8 completado\n",
            "\n",
            "Episode 9 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 3, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n",
            "Episodio: 6, Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 9 completado\n",
            "\n",
            "Episode 10 iniciado\n",
            "Episodio: 1, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 2, Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n",
            "Episodio: 3, Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 4, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 5, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 6, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 7, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 8, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 9, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episodio: 10, Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n",
            "Episode 10 completado\n"
          ]
        }
      ],
      "source": [
        "#Importacion de libreria random: Utilizada para generar números aleatorios, lo que permite al agente tomar decisiones aleatorias o explorar el entorno.\n",
        "import random\n",
        "#Impotacion de libreria numpy: numpy: Una biblioteca para realizar operaciones matemáticas complejas, especialmente útil para trabajar con arrays y matrices. Aquí se usa para manejar la tabla Q.\n",
        "import numpy as np\n",
        "\n",
        "class Environment:\n",
        "  #__init__(self): Inicializa el entorno, estableciendo la posición del objetivo y reseteando el entorno para un nuevo episodio.\n",
        "    def __init__(self):\n",
        "        self.goal_position = 4  # Define el objetivo de manera más explícita\n",
        "        self.reset()\n",
        "  #reset(self): Reinicia el entorno al estado inicial, útil al comenzar un nuevo episodio.\n",
        "    def reset(self):\n",
        "        self.steps_left = 10\n",
        "        self.board = [False] * 5\n",
        "        self.board[self.goal_position] = True\n",
        "  #is_done(self): Verifica si el entorno ha llegado a un estado terminal (por ejemplo, se han agotado los pasos disponibles).\n",
        "    def is_done(self):\n",
        "        return self.steps_left == 0\n",
        "  #action(self, state):  Ejecuta una acción (avanzar o retroceder) y retorna si la acción resulta en alcanzar la posición objetivo. También decrementa el contador de pasos.\n",
        "    def action(self, state):\n",
        "        self.steps_left -= 1\n",
        "        return self.board[state] if 0 <= state < len(self.board) else False\n",
        "\n",
        "class Agent:\n",
        "  #__init__(self): Inicializa el estado del agente.\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "  #reset(self): Reinicia el estado del agente a su posición inicial.\n",
        "    def reset(self):\n",
        "        self.state = 0\n",
        "  # perform_action(self, action, env): Realiza una acción en el entorno y actualiza el estado del agente basado en la acción elegida.\n",
        "\n",
        "    def perform_action(self, action, env):\n",
        "        movement = {1: 1, 0: -1}.get(action, 0)  # Mapea la acción a un cambio de estado\n",
        "        self.state = max(0, min(self.state + movement, env.goal_position))\n",
        "        return env.action(self.state)\n",
        "  #get_action_name(action): Método estático que traduce la acción numérica a una cadena de texto (\"avanzar\" o \"retroceder\") para facilitar la interpretación.\n",
        "    @staticmethod\n",
        "    def get_action_name(action):\n",
        "        return \"avanzar\" if action == 1 else \"retroceder\"\n",
        "\n",
        "class Learner:\n",
        "  # __init__(self, agent, env, alpha, gamma, epsilon): Inicializa el aprendiz con referencias al agente y al entorno, junto con los parámetros de aprendizaje: tasa de aprendizaje (alpha), factor de descuento (gamma), y la tasa de exploración (epsilon).\n",
        "    def __init__(self, agent, env, alpha=0.1, gamma=0.6, epsilon=0.1):\n",
        "        self.agent = agent\n",
        "        self.env = env\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon\n",
        "        self.q_table = np.zeros((5, 2))  # 5 estados y 2 acciones\n",
        "  # run_episode(self): Ejecuta un episodio completo de entrenamiento, donde el agente toma acciones ya sea exploratoriamente (aleatoriamente) o explotando su conocimiento actual (la acción con el mayor valor Q estimado). Actualiza la tabla Q basada en las recompensas obtenidas y las transiciones de estado.\n",
        "    def run_episode(self):\n",
        "        self.env.reset()\n",
        "        self.agent.reset()\n",
        "        step = 0\n",
        "        while not self.env.is_done():\n",
        "            step += 1\n",
        "            current_state = self.agent.state\n",
        "            action = random.choice([0, 1]) if random.uniform(0, 1) < self.epsilon else np.argmax(self.q_table[current_state])\n",
        "            reward = self.agent.perform_action(action, self.env)\n",
        "            next_state = self.agent.state\n",
        "            self.q_table[current_state, action] = (1 - self.alpha) * self.q_table[current_state, action] + \\\n",
        "                                                  self.alpha * (reward + self.gamma * np.max(self.q_table[next_state]))\n",
        "            print(f\"Episodio: {step}, Estado actual: {current_state}, Acción: {self.agent.get_action_name(action)}, Nuevo estado: {next_state}\")\n",
        "\n",
        "#Orquestar procesamiento:\n",
        "def main():\n",
        "    episodes = 10\n",
        "    env = Environment()\n",
        "    agent = Agent()\n",
        "    learner = Learner(agent, env)\n",
        "\n",
        "    for episode in range(1, episodes + 1):\n",
        "        print(f\"\\nEpisode {episode} iniciado\")\n",
        "        learner.run_episode()\n",
        "        print(f\"Episode {episode} completado\")\n",
        "\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
